# langgraph_simple_agent.py
# Program demonstrates use of LangGraph for a very simple agent.
# It writes to stdout and asks the user to enter a line of text through stdin.
# It passes the line to the LLM llama-3.2-1B-Instruct, then prints the
# what the LLM returns as text to stdout.
# The LLM should use Cuda if available, if not then if mps is available then use that,
# otherwise use cpu.
# After the LangGraph graph is created but before it executes, the program
# uses the Mermaid library to write a image of the graph to the file lg_graph.png
# The program gets the LLM llama-3.2-1B-Instruct from Hugging Face and wraps
# it for LangChain using HuggingFacePipeline.
# The code is commented in detail so a reader can understand each step.

# Import necessary libraries
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

# Determine the best available device for inference
# Priority: CUDA (NVIDIA GPU) > MPS (Apple Silicon) > CPU
def get_device():
    """
    Detect and return the best available compute device.
    Returns 'cuda' for NVIDIA GPUs, 'mps' for Apple Silicon, or 'cpu' as fallback.
    """
    if torch.cuda.is_available():
        print("Using CUDA (NVIDIA GPU) for inference")
        return "cuda"
    elif torch.backends.mps.is_available():
        print("Using MPS (Apple Silicon) for inference")
        return "mps"
    else:
        print("Using CPU for inference")
        return "cpu"

# =============================================================================
# STATE DEFINITION
# =============================================================================
# The state is a TypedDict that flows through all nodes in the graph.
# Each node can read from and write to specific fields in the state.
# LangGraph automatically merges the returned dict from each node into the state.

class AgentState(TypedDict):
    """
    State object that flows through the LangGraph nodes.

    Fields:
    - user_input: The text entered by the user (set by get_user_input node)
    - should_exit: Boolean flag indicating if user wants to quit (set by get_user_input node)
    - llama_response: The response generated by the Llama model (set by call_llama node)
    - qwen_response: The response generated by the Qwen model (set by call_qwen node)
    - verbose: Boolean flag indicating if verbose tracing is enabled (set by get_user_input node)
    - skip_llm: Boolean flag indicating if LLM should be skipped (for verbose/quiet commands)
    - is_empty_input: Boolean flag indicating if user input is empty/whitespace (set by get_user_input node)

    State Flow:
    1. Initial state: all fields empty/default
    2. After get_user_input: user_input, should_exit, verbose, skip_llm, and is_empty_input are populated
    3. After call_llama OR call_qwen (single model): llama_response or qwen_response is populated
    4. After print_both_responses: state unchanged (node only reads, doesn't write)

    The graph loops continuously with conditional routing from get_user_input:
        get_user_input -> [conditional] -> get_user_input (if empty input - loop back)
                              |
                              +-> END (if user wants to quit)
                              +-> call_qwen (if input starts with "Hey Qwen")
                              +-> call_llama (otherwise, default)
                              +-> print_response (if skip_llm, bypassing LLMs)
    """
    user_input: str
    should_exit: bool
    llama_response: str
    qwen_response: str
    verbose: bool
    skip_llm: bool
    is_empty_input: bool

def create_llama_llm():
    """
    Create and configure the Llama LLM using HuggingFace's transformers library.
    Downloads llama-3.2-1B-Instruct from HuggingFace Hub and wraps it
    for use with LangChain via HuggingFacePipeline.
    """
    # Get the optimal device for inference
    device = get_device()

    # Model identifier on HuggingFace Hub
    model_id = "meta-llama/Llama-3.2-1B-Instruct"

    print(f"Loading Llama model: {model_id}")
    print("This may take a moment on first run as the model is downloaded...")

    # Load the tokenizer - converts text to tokens the model understands
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Load the model itself with appropriate settings for the device
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        dtype=torch.float16 if device != "cpu" else torch.float32,
        device_map=device if device == "cuda" else None,
    )

    # Move model to MPS device if using Apple Silicon
    if device == "mps":
        model = model.to(device)

    # Create a text generation pipeline that combines model and tokenizer
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,  # Maximum tokens to generate in response
        do_sample=True,      # Enable sampling for varied responses
        temperature=0.7,     # Controls randomness (lower = more deterministic)
        top_p=0.95,          # Nucleus sampling parameter
        pad_token_id=tokenizer.eos_token_id,  # Suppress pad_token_id warning
    )

    # Wrap the HuggingFace pipeline for use with LangChain
    llm = HuggingFacePipeline(pipeline=pipe)

    print("Llama model loaded successfully!")
    return llm

def create_qwen_llm():
    """
    Create and configure the Qwen LLM using HuggingFace's transformers library.
    Downloads Qwen2.5-0.5B-Instruct from HuggingFace Hub and wraps it
    for use with LangChain via HuggingFacePipeline.
    """
    # Get the optimal device for inference
    device = get_device()

    # Model identifier on HuggingFace Hub - using a small Qwen model
    model_id = "Qwen/Qwen2.5-0.5B-Instruct"

    print(f"Loading Qwen model: {model_id}")
    print("This may take a moment on first run as the model is downloaded...")

    # Load the tokenizer - converts text to tokens the model understands
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Load the model itself with appropriate settings for the device
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        dtype=torch.float16 if device != "cpu" else torch.float32,
        device_map=device if device == "cuda" else None,
    )

    # Move model to MPS device if using Apple Silicon
    if device == "mps":
        model = model.to(device)

    # Create a text generation pipeline that combines model and tokenizer
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,  # Maximum tokens to generate in response
        do_sample=True,      # Enable sampling for varied responses
        temperature=0.7,     # Controls randomness (lower = more deterministic)
        top_p=0.95,          # Nucleus sampling parameter
        pad_token_id=tokenizer.eos_token_id,  # Suppress pad_token_id warning
    )

    # Wrap the HuggingFace pipeline for use with LangChain
    llm = HuggingFacePipeline(pipeline=pipe)

    print("Qwen model loaded successfully!")
    return llm

def create_graph(llama_llm, qwen_llm):
    """
    Create the LangGraph state graph with nodes for single model execution:
    1. get_user_input: Reads input from stdin
    2. call_llama: Sends input to the Llama model (default)
    3. call_qwen: Sends input to the Qwen model (if input starts with "Hey Qwen")
    4. print_both_responses: Prints the model response to stdout
    5. print_response: Prints messages for verbose/quiet commands

    Graph structure with conditional routing based on input prefix:
        START -> get_user_input -> [conditional] -> call_llama -> print_both_responses -+
                       ^                 |              |                                |
                       |                 |              +-> call_qwen -> print_both_responses
                       |                 +-> END (if user wants to quit)                 |
                       |                 +-> print_response (if skip_llm)               |
                       |                                                                 |
                       +-----------------------------------------------------------------+

    The graph runs continuously until the user types 'quit', 'exit', or 'q'.
    Users can type 'verbose' to enable tracing or 'quiet' to disable it.
    If input starts with "Hey Qwen", routes to Qwen model; otherwise routes to Llama.
    """

    # =========================================================================
    # NODE 1: get_user_input
    # =========================================================================
    # This node reads a line of text from stdin and updates the state.
    # State changes:
    #   - user_input: Set to the text entered by the user
    #   - should_exit: Set to True if user typed quit/exit/q, False otherwise
    #   - verbose: Set to True if user typed "verbose", False if "quiet", otherwise unchanged
    #   - skip_llm: Set to True if user typed "verbose" or "quiet" (these are commands, not LLM inputs)
    def get_user_input(state: AgentState) -> dict:
        """
        Node that prompts the user for input via stdin.

        Reads state:
            - verbose: Current verbose mode setting (to preserve it)
        Updates state:
            - user_input: The raw text entered by the user
            - should_exit: True if user wants to quit, False otherwise
            - verbose: Updated if user typed "verbose" or "quiet"
            - skip_llm: True if user typed "verbose" or "quiet" (commands, not LLM inputs)
            - is_empty_input: True if user input is empty or only whitespace
        """
        verbose = state.get("verbose", False)
        
        # Print tracing info if verbose mode is enabled
        if verbose:
            print("\n[TRACE] Entering node: get_user_input")
            print(f"[TRACE] Current state - verbose: {verbose}, should_exit: {state.get('should_exit', False)}")
        
        # Display banner before each prompt
        print("\n" + "=" * 50)
        print("Enter your text (or 'quit' to exit, 'verbose' for tracing, 'quiet' to disable tracing):")
        print("=" * 50)

        print("\n> ", end="")
        user_input = input()

        # Check if input is empty or only whitespace
        is_empty = not user_input.strip()
        
        if is_empty:
            if verbose:
                print("[TRACE] Empty input detected - will loop back to get_user_input")
            print("Please enter some text (empty input ignored).")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": verbose,         # Preserve verbose setting
                "skip_llm": False,          # Not relevant for empty input
                "is_empty_input": True      # Signal that input is empty
            }

        # Check if user wants to exit
        if user_input.lower() in ['quit', 'exit', 'q']:
            if verbose:
                print("[TRACE] User requested exit")
            print("Goodbye!")
            return {
                "user_input": user_input,
                "should_exit": True,        # Signal to exit the graph
                "verbose": verbose,         # Preserve verbose setting
                "skip_llm": False,          # Not relevant for exit
                "is_empty_input": False     # Not empty
            }

        # Check if user wants to toggle verbose mode
        if user_input.lower() == "verbose":
            if verbose:
                print("[TRACE] Verbose mode already enabled")
            else:
                print("Verbose tracing enabled")
            if verbose:
                print("[TRACE] Setting verbose=True, skip_llm=True")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": True,            # Enable verbose mode
                "skip_llm": True,           # Skip LLM for this command
                "is_empty_input": False    # Not empty
            }
        
        if user_input.lower() == "quiet":
            if verbose:
                print("[TRACE] Disabling verbose mode")
            else:
                print("Verbose tracing disabled")
            if verbose:
                print("[TRACE] Setting verbose=False, skip_llm=True")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": False,           # Disable verbose mode
                "skip_llm": True,           # Skip LLM for this command
                "is_empty_input": False    # Not empty
            }

        # Any other input - continue to LLM
        if verbose:
            print(f"[TRACE] User input received: '{user_input}'")
            print("[TRACE] Setting skip_llm=False (will proceed to LLM)")
        return {
            "user_input": user_input,
            "should_exit": False,           # Signal to proceed to LLM
            "verbose": verbose,             # Preserve verbose setting
            "skip_llm": False,              # Proceed to LLM
            "is_empty_input": False        # Not empty
        }

    # =========================================================================
    # NODE 2: call_llama
    # =========================================================================
    # This node takes the user input from state, sends it to the Llama model,
    # and stores the response back in state. Only one model runs per input.
    # State changes:
    #   - user_input: Unchanged (read only)
    #   - llama_response: Set to the Llama model's generated response
    def call_llama(state: AgentState) -> dict:
        """
        Node that invokes the Llama model with the user's input.
        Only one model (Llama or Qwen) runs per input based on input prefix.

        Reads state:
            - user_input: The text to send to the Llama model
            - verbose: Whether to print tracing information
        Updates state:
            - llama_response: The text generated by the Llama model
        """
        verbose = state.get("verbose", False)
        user_input = state["user_input"]

        # Print tracing info if verbose mode is enabled
        if verbose:
            print("\n[TRACE] Entering node: call_llama")
            print(f"[TRACE] Current state - user_input: '{user_input}', verbose: {verbose}")

        # Format the prompt for the instruction-tuned model
        prompt = f"User: {user_input}\nAssistant:"

        if verbose:
            print(f"[TRACE] Formatted prompt for Llama: '{prompt}'")
            print("[TRACE] Invoking Llama model...")

        print("\nProcessing your input with Llama...")

        # Invoke the Llama model and get the response
        response = llama_llm.invoke(prompt)

        if verbose:
            print(f"[TRACE] Llama response received: '{response}'")
            print("[TRACE] Exiting node: call_llama")

        # Return only the field we're updating
        return {"llama_response": response}

    # =========================================================================
    # NODE 3: call_qwen
    # =========================================================================
    # This node takes the user input from state, sends it to the Qwen model,
    # and stores the response back in state. Only one model runs per input.
    # State changes:
    #   - user_input: Unchanged (read only)
    #   - qwen_response: Set to the Qwen model's generated response
    def call_qwen(state: AgentState) -> dict:
        """
        Node that invokes the Qwen model with the user's input.
        Runs in parallel with call_llama.

        Reads state:
            - user_input: The text to send to the Qwen model
            - verbose: Whether to print tracing information
        Updates state:
            - qwen_response: The text generated by the Qwen model
        """
        verbose = state.get("verbose", False)
        user_input = state["user_input"]

        # Print tracing info if verbose mode is enabled
        if verbose:
            print("\n[TRACE] Entering node: call_qwen")
            print(f"[TRACE] Current state - user_input: '{user_input}', verbose: {verbose}")

        # Format the prompt for the instruction-tuned model
        prompt = f"User: {user_input}\nAssistant:"

        if verbose:
            print(f"[TRACE] Formatted prompt for Qwen: '{prompt}'")
            print("[TRACE] Invoking Qwen model...")

        print("\nProcessing your input with Qwen...")

        # Invoke the Qwen model and get the response
        response = qwen_llm.invoke(prompt)

        if verbose:
            print(f"[TRACE] Qwen response received: '{response}'")
            print("[TRACE] Exiting node: call_qwen")

        # Return only the field we're updating
        return {"qwen_response": response}

    # =========================================================================
    # NODE 4: print_both_responses
    # =========================================================================
    # This node reads both model responses from state and prints them to stdout.
    # This node prints the response from whichever model was executed (single model execution).
    # State changes:
    #   - No changes (this node only reads state, doesn't modify it)
    def print_both_responses(state: AgentState) -> dict:
        """
        Node that prints the model response to stdout.
        Prints whichever model response is available (only one model runs at a time).

        Reads state:
            - llama_response: The text generated by Llama model (if Llama was used)
            - qwen_response: The text generated by Qwen model (if Qwen was used)
            - verbose: Whether to print tracing information
        Updates state:
            - Nothing (returns empty dict, state unchanged)
        """
        verbose = state.get("verbose", False)

        # Print tracing info if verbose mode is enabled
        if verbose:
            print("\n[TRACE] Entering node: print_both_responses")
            print(f"[TRACE] Current state - verbose: {verbose}")

        llama_response = state.get("llama_response", "")
        qwen_response = state.get("qwen_response", "")

        print("\n" + "=" * 50)
        print("MODEL RESPONSE")
        print("=" * 50)
        
        # Print whichever response exists
        if llama_response:
            print("\n" + "-" * 50)
            print("Llama Response:")
            print("-" * 50)
            print(llama_response)
        elif qwen_response:
            print("\n" + "-" * 50)
            print("Qwen Response:")
            print("-" * 50)
            print(qwen_response)
        else:
            print("\nNo response available")
        
        print("\n" + "=" * 50)

        if verbose:
            print("[TRACE] Response printed successfully")
            print("[TRACE] Exiting node: print_both_responses")

        # Return empty dict - no state updates from this node
        return {}

    # =========================================================================
    # NODE 5: print_response
    # =========================================================================
    # This node handles printing for verbose/quiet commands (when LLMs are skipped).
    # State changes:
    #   - No changes (this node only reads state, doesn't modify it)
    def print_response(state: AgentState) -> dict:
        """
        Node that prints messages for verbose/quiet commands (when LLMs are skipped).

        Reads state:
            - verbose: Whether to print tracing information
            - skip_llm: Whether LLM was skipped (for verbose/quiet commands)
        Updates state:
            - Nothing (returns empty dict, state unchanged)
        """
        verbose = state.get("verbose", False)
        skip_llm = state.get("skip_llm", False)

        # Print tracing info if verbose mode is enabled
        if verbose:
            print("\n[TRACE] Entering node: print_response")
            print(f"[TRACE] Current state - skip_llm: {skip_llm}, verbose: {verbose}")

        # If LLM was skipped (verbose/quiet commands), don't print LLM response
        if skip_llm:
            if verbose:
                print("[TRACE] LLMs were skipped (verbose/quiet command), no response to print")
                print("[TRACE] Exiting node: print_response")
            return {}

        if verbose:
            print("[TRACE] Exiting node: print_response")

        # Return empty dict - no state updates from this node
        return {}

    # =========================================================================
    # ROUTING FUNCTION
    # =========================================================================
    # This function examines the state and determines which node to go to next.
    # It's used for conditional edges after get_user_input.
    # Three-way conditional routing:
    #   1. Empty input -> loop back to get_user_input
    #   2. User wants to quit -> END
    #   3. User entered valid input -> proceed to call_llm or print_response
    def route_after_input(state: AgentState):
        """
        Routing function that determines the next node based on state.
        Implements conditional branching from get_user_input.
        Routes to a single model based on user input prefix.

        Examines state:
            - is_empty_input: If True, loop back to get_user_input
            - should_exit: If True, terminate the graph
            - skip_llm: If True, skip LLM and go directly to print_response
            - user_input: If starts with "Hey Qwen", route to Qwen, otherwise to Llama

        Returns:
            - "get_user_input": If input is empty (loop back)
            - "__end__": If user wants to quit
            - "print_response": If user typed verbose/quiet (skip LLM)
            - "call_qwen": If user input starts with "Hey Qwen"
            - "call_llama": Otherwise (default)
        """
        verbose = state.get("verbose", False)
        
        if verbose:
            print("\n[TRACE] Routing function: route_after_input")
            print(f"[TRACE] is_empty_input: {state.get('is_empty_input', False)}")
            print(f"[TRACE] should_exit: {state.get('should_exit', False)}, skip_llm: {state.get('skip_llm', False)}")
        
        # First check: Empty input -> loop back to get_user_input
        if state.get("is_empty_input", False):
            if verbose:
                print("[TRACE] Routing to: get_user_input (empty input - loop back)")
            return "get_user_input"
        
        # Second check: User wants to exit
        if state.get("should_exit", False):
            if verbose:
                print("[TRACE] Routing to: END")
            return END

        # Third check: Should skip LLM (verbose/quiet commands)
        if state.get("skip_llm", False):
            if verbose:
                print("[TRACE] Routing to: print_response (skipping LLM)")
            return "print_response"

        # Fourth check: Route to Qwen if input starts with "Hey Qwen" (case-insensitive)
        user_input = state.get("user_input", "")
        if user_input.strip().lower().startswith("hey qwen"):
            if verbose:
                print("[TRACE] Input starts with 'Hey Qwen' - routing to: call_qwen")
            return "call_qwen"
        
        # Default: Route to Llama for all other inputs
        if verbose:
            print("[TRACE] Routing to: call_llama (default)")
        return "call_llama"

    # =========================================================================
    # GRAPH CONSTRUCTION
    # =========================================================================
    # Create a StateGraph with our defined state structure
    graph_builder = StateGraph(AgentState)

    # Add all nodes to the graph
    graph_builder.add_node("get_user_input", get_user_input)
    graph_builder.add_node("call_llama", call_llama)
    graph_builder.add_node("call_qwen", call_qwen)
    graph_builder.add_node("print_both_responses", print_both_responses)
    graph_builder.add_node("print_response", print_response)

    # Define edges:
    # 1. START -> get_user_input (always start by getting user input)
    graph_builder.add_edge(START, "get_user_input")

    # 2. get_user_input -> [conditional] -> get_user_input OR call_llama OR call_qwen OR print_response OR END
    #    Uses route_after_input to decide based on input prefix:
    #    - If input starts with "Hey Qwen" -> call_qwen
    #    - Otherwise -> call_llama
    graph_builder.add_conditional_edges(
        "get_user_input",      # Source node
        route_after_input,      # Routing function that examines state and input prefix
        {
            "get_user_input": "get_user_input", # Empty input -> loop back to itself
            "call_llama": "call_llama",         # Default: route to Llama
            "call_qwen": "call_qwen",           # If input starts with "Hey Qwen"
            "print_response": "print_response", # Verbose/quiet -> skip LLMs
            END: END                            # Quit command -> terminate graph
        }
    )

    # 3. Both call_llama and call_qwen -> print_both_responses
    #    Only one will execute per input, but both can route to the same print node
    graph_builder.add_edge("call_llama", "print_both_responses")
    graph_builder.add_edge("call_qwen", "print_both_responses")

    # 4. print_both_responses -> get_user_input (loop back for next input)
    #    This creates the continuous loop - after printing, go back to get more input
    graph_builder.add_edge("print_both_responses", "get_user_input")

    # 5. print_response -> get_user_input (loop back for next input after verbose/quiet commands)
    graph_builder.add_edge("print_response", "get_user_input")

    # Compile the graph into an executable form
    graph = graph_builder.compile()

    return graph

def save_graph_image(graph, filename="lg_graph.png"):
    """
    Generate a Mermaid diagram of the graph and save it as a PNG image.
    Uses the graph's built-in Mermaid export functionality.
    """
    try:
        # Get the Mermaid PNG representation of the graph
        # This requires the 'grandalf' package for rendering
        png_data = graph.get_graph(xray=True).draw_mermaid_png()

        # Write the PNG data to file
        with open(filename, "wb") as f:
            f.write(png_data)

        print(f"Graph image saved to {filename}")
    except Exception as e:
        print(f"Could not save graph image: {e}")
        print("You may need to install additional dependencies: pip install grandalf")

def save_graph_mermaid(graph, filename="lg_graph.mmd"):
    """
    Save the graph as Mermaid diagram text format.
    This can be viewed online at https://mermaid.live/ or in many markdown viewers.
    """
    try:
        # Get the Mermaid text representation
        mermaid_text = graph.get_graph(xray=True).draw_mermaid()
        
        # Write to file
        with open(filename, "w") as f:
            f.write(mermaid_text)
        
        print(f"Mermaid diagram saved to {filename}")
        print(f"You can view it online at: https://mermaid.live/")
        print(f"Or copy the contents and paste into any Mermaid-compatible viewer")
        return mermaid_text
    except Exception as e:
        print(f"Could not save Mermaid diagram: {e}")
        return None

def print_graph_structure(graph):
    """
    Print the graph structure as text for quick inspection.
    """
    try:
        graph_structure = graph.get_graph(xray=True)
        print("\n" + "=" * 50)
        print("Graph Structure:")
        print("=" * 50)
        print(graph_structure.draw_ascii())
        print("=" * 50 + "\n")
    except Exception as e:
        print(f"Could not print graph structure: {e}")

def visualize_graph(graph, save_png=True, save_mermaid=True, print_ascii=True):
    """
    Comprehensive graph visualization function.
    
    Args:
        graph: The compiled LangGraph
        save_png: Whether to save PNG image (requires grandalf)
        save_mermaid: Whether to save Mermaid text file
        print_ascii: Whether to print ASCII representation
    """
    print("\n" + "=" * 50)
    print("Graph Visualization")
    print("=" * 50)
    
    # Print ASCII representation
    if print_ascii:
        print_graph_structure(graph)
    
    # Save Mermaid text file (most reliable)
    if save_mermaid:
        mermaid_text = save_graph_mermaid(graph)
        if mermaid_text:
            print(f"\nMermaid code preview (first 500 chars):")
            print("-" * 50)
            print(mermaid_text[:500] + "..." if len(mermaid_text) > 500 else mermaid_text)
            print("-" * 50)
    
    # Save PNG image (requires grandalf)
    if save_png:
        save_graph_image(graph)
    
    print("=" * 50 + "\n")

def main():
    """
    Main function that orchestrates the simple agent workflow:
    1. Initialize the LLM
    2. Create the LangGraph
    3. Save the graph visualization
    4. Run the graph once (it loops internally until user quits)

    The graph handles all looping internally through its edge structure:
    - get_user_input: Prompts and reads from stdin
    - call_llm: Processes input through the LLM
    - print_response: Outputs the response, then loops back to get_user_input

    The graph only terminates when the user types 'quit', 'exit', or 'q'.
    """
    print("=" * 50)
    print("LangGraph Agent with Llama and Qwen Models")
    print("=" * 50)
    print()

    # Step 1: Create and configure both LLMs
    print("Loading models...")
    llama_llm = create_llama_llm()
    qwen_llm = create_qwen_llm()

    # Step 2: Build the LangGraph with both LLMs
    print("\nCreating LangGraph...")
    graph = create_graph(llama_llm, qwen_llm)
    print("Graph created successfully!")

    # Step 3: Save a visual representation of the graph before execution
    # This happens BEFORE any graph execution, showing the graph structure
    print("\nGenerating graph visualization...")
    # Save with unique filenames for number4
    save_graph_image(graph, filename="lg_graph_number4.png")
    save_graph_mermaid(graph, filename="lg_graph_number4.mmd")
    print_graph_structure(graph)

    # Step 4: Run the graph - it will loop internally until user quits
    # Create initial state with empty/default values
    # The graph will loop continuously, updating state as it goes:
    #   - get_user_input displays banner, populates user_input, should_exit, verbose, and skip_llm
    #   - call_llm populates llm_response
    #   - print_response displays output, then loops back to get_user_input
    initial_state: AgentState = {
        "user_input": "",
        "should_exit": False,
        "llama_response": "",
        "qwen_response": "",
        "verbose": False,      # Start with verbose mode disabled
        "skip_llm": False,     # Start with normal flow
        "is_empty_input": False # Start with non-empty (will be set by get_user_input)
    }

    # Single invocation - the graph loops internally via print_response -> get_user_input
    # The graph only exits when route_after_input returns END (user typed quit/exit/q)
    graph.invoke(initial_state)

# Entry point - only run main() if this script is executed directly
if __name__ == "__main__":
    main()
